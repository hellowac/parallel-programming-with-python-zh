##爬取网页

贯穿本书的另一个正在研究的问题是是实现一个并行的网络爬虫。一个网络爬虫由一个个浏览网页并从页面获取信息的电脑程序组成。被分析的场景是一个问题，在这个问题中，一个顺序执行的网络爬虫会被提供一组可变数量的统一资源定位器（URLs）为参数，它需要检索每个URL所提供的所有链接。假设输入的URLs的数量相当的大，我们可以在以下方法中寻找并行性的解决方案：

1. 将所有的URLs分成组组合到一个数据结构中。
2. 把这些URLs分配給多个任务，这样写任务会爬取每个URL中的包含信息。
3. 将这些任务分派给多个并行的workers来执行。
4. 由于前一阶段的结果必须传给下一个阶段，这将会改进未加工的存储的数据，因此保存它们关联它们到原始的URLs。

正如我们在上面编号的步骤中观察到的为了一个提出的解决方案，可以与以下两个方法结合：

* 数据分解：这发生在我们划分和关联URLs到任务上。
* 用管道进行任务分解：这包含三个阶段的管道，这发生在我们链接接收、存储以及组织爬取的结果的任务。

下图显示了解决方案：

![](https://github.com/Voidly/Img/blob/master/Parallel%20Programming%20with%20Python/Chapter%203/Parallel%20Web%20crawler.png?raw=true)
